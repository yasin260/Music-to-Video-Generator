# -*- coding: utf-8 -*-
"""Music-to-Video Generator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_k51lRdnFJ7gAkZ-ltNFKRqPUANcavQb

# ğŸµ Music-to-Video Generation using AI

# ğŸ¬ Ø¹Ù†ÙˆØ§Ù† Ù¾Ø±ÙˆÚ˜Ù‡: Music-to-Video Generator  


### ğŸ« Ø¯Ø±Ø³  
**Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ù†Ù‡ÙØªÙ‡ Ùˆ Ø¨ÛŒâ€ŒØ¯Ø±Ù†Ú¯**

### ğŸ‘¨â€ğŸ« Ø§Ø³ØªØ§Ø¯  
**Ù…Ù‡Ù†Ø¯Ø³ Ù…Ù‡Ø¯ÛŒ Ø³ÛŒÙÛŒâ€ŒÙ¾ÙˆØ±**

### ğŸ‘¥ Ø§Ø¹Ø¶Ø§ÛŒ ØªÛŒÙ…  
- Ø³ÛŒØ¯ Ù…Ù‡Ø¯ÛŒ Ù…Ù†Ø¬Ù…  
- Ø¹Ù„ÛŒØ±Ø¶Ø§ Ù…ÛŒØ±Ø²Ø§ÛŒÛŒ  
- Ù…Ø­Ù…Ø¯Ø­Ø³ÛŒÙ† ÙØ±Ù‡Ø§Ø¯ÛŒØ§Ù†  

### ğŸ“… ØªØ§Ø±ÛŒØ®  
ØªØ§Ø¨Ø³ØªØ§Ù† 1404  

---

## âœ¨ Ú†Ú©ÛŒØ¯Ù‡  

Ø¯Ø± Ø§ÛŒÙ† Ù¾Ø±ÙˆÚ˜Ù‡ØŒ Ø³Ø§Ù…Ø§Ù†Ù‡â€ŒØ§ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø·Ø±Ø§Ø­ÛŒ Ùˆ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª Ú©Ù‡ Ù‚Ø§Ø¨Ù„ÛŒØª **ØªØ¨Ø¯ÛŒÙ„ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ Ù…ÙˆØ³ÛŒÙ‚ÛŒ Ø¨Ù‡ ÙˆÛŒØ¯Ø¦Ùˆ** Ø±Ø§ ÙØ±Ø§Ù‡Ù… Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯.  
ÙØ±Ø¢ÛŒÙ†Ø¯ Ú©Ù„ÛŒ Ø´Ø§Ù…Ù„ Ú†Ù‡Ø§Ø± Ù…Ø±Ø­Ù„Ù‡â€ŒÛŒ Ø§ØµÙ„ÛŒ Ø§Ø³Øª:  
1. **Ù¾Ø±Ø¯Ø§Ø²Ø´ ØµÙˆØª** Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† (Ø§Ø´Ø¹Ø§Ø±) **Whisper**  
2. **ØªÙˆÙ„ÛŒØ¯ Ù¾Ø±Ø§Ù…Ù¾Øª Ø®Ù„Ø§Ù‚Ø§Ù†Ù‡** Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…ØªÙ† Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡ ØªÙˆØ³Ø· Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù†ÛŒ **Gemini**  
3. **ØªÙˆÙ„ÛŒØ¯ ØªØµÙˆÛŒØ± Ø§ÙˆÙ„ÛŒÙ‡** **Stable Diffusion XL**  
4. **ØªØ¨Ø¯ÛŒÙ„ ØªØµÙˆÛŒØ± Ø¨Ù‡ ÙˆÛŒØ¯Ø¦Ùˆ** **Stable Video Diffusion**  

Ø®Ø±ÙˆØ¬ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø³Ø§Ù…Ø§Ù†Ù‡ØŒ ÙˆÛŒØ¯ÛŒÙˆÛŒÛŒ Ø§Ø³Øª Ú©Ù‡ Ø¨Ø§ Ù…Ø­ØªÙˆØ§ÛŒ Ù…ÙˆØ³ÛŒÙ‚ÛŒ ÙˆØ±ÙˆØ¯ÛŒ Ù‡Ù…Ø§Ù‡Ù†Ú¯ Ø¨ÙˆØ¯Ù‡ Ùˆ Ø¬Ù†Ø¨Ù‡â€ŒÙ‡Ø§ÛŒ Ù‡Ù†Ø±ÛŒØŒ Ø§Ø­Ø³Ø§Ø³ÛŒ Ùˆ Ø¨ØµØ±ÛŒ Ø¢Ù† Ø±Ø§ Ø¨Ù‡â€ŒØµÙˆØ±Øª Ø®Ù„Ø§Ù‚Ø§Ù†Ù‡ Ø¨Ø§Ø²Ø¢ÙØ±ÛŒÙ†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.  

Ø§ÛŒÙ† Ø³ÛŒØ³ØªÙ… Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² **Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ Ù…ÛŒØ§Ù†â€ŒØ±Ø´ØªÙ‡â€ŒØ§ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ** Ø¯Ø± ØªØ±Ú©ÛŒØ¨ Ø­ÙˆØ²Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒØŒ Ø¨ÛŒÙ†Ø§ÛŒÛŒ Ù…Ø§Ø´ÛŒÙ† Ùˆ ØªÙˆÙ„ÛŒØ¯ Ù…Ø­ØªÙˆØ§ÛŒ Ú†Ù†Ø¯Ø±Ø³Ø§Ù†Ù‡â€ŒØ§ÛŒ Ø¨Ù‡ Ø´Ù…Ø§Ø± Ù…ÛŒâ€ŒØ±ÙˆØ¯.
"""

!pip install pydub whisper-openai google-generativeai python-dotenv diffusers transformers accelerate torch torchvision imageio[ffmpeg]
!pip install --upgrade Pillow

from google.colab import userdata
GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')

"""## ğŸ“Œ Ú¯Ø§Ù… ÛŒÚ©: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ†  
Ø¯Ø± Ù†Ø®Ø³ØªÛŒÙ† Ú¯Ø§Ù…ØŒ ÙØ§ÛŒÙ„ Ù…ÙˆØ³ÛŒÙ‚ÛŒ ÙˆØ±ÙˆØ¯ÛŒ Ø¯Ø± Ù‚Ø§Ù„Ø¨ **Ø§Ù… Ù¾ÛŒ ØªØ±ÛŒ** Ø¯Ø±ÛŒØ§ÙØª Ùˆ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´â€ŒÙ‡Ø§ÛŒ Ø¨Ø¹Ø¯ÛŒ Ø¨Ù‡ ÙØ±Ù…Øª **ÙˆÛŒÙˆ** ØªØ¨Ø¯ÛŒÙ„ Ø´Ø¯.  
Ø³Ù¾Ø³ Ø§ÛŒÙ† ÙØ§ÛŒÙ„ Ø¨Ù‡ Ù…Ø¯Ù„ **ÙˆÛŒØ³Ù¾Ø±** Ø¯Ø§Ø¯Ù‡ Ø´Ø¯ ØªØ§ Ù…Ø­ØªÙˆØ§ÛŒ ØµÙˆØªÛŒ Ø¢Ù† ØªØ­Ù„ÛŒÙ„ Ø´Ø¯Ù‡ Ùˆ Ù…ØªÙ† (Ø§Ø´Ø¹Ø§Ø±) Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù…ÙˆØ³ÛŒÙ‚ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú¯Ø±Ø¯Ø¯.  
Ø¯Ø± Ù†Ù‡Ø§ÛŒØª Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² **Ø¬Ù…ÛŒÙ†ÛŒ** Ù¾Ø±Ø§Ù…Ù¾Øª Ù…ØªÙ†Ø§Ø³Ø¨ Ø¨Ø§ Ù…ÙˆØ³ÛŒÙ‚ÛŒ ØªÙˆÙ„ÛŒØ¯ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.

Ø§ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ØŒ Ù¾Ø§ÛŒÙ‡ Ùˆ Ø§Ø³Ø§Ø³ Ø³Ø§ÛŒØ± Ù…Ø±Ø§Ø­Ù„ Ù¾Ø±ÙˆÚ˜Ù‡ Ù…Ø­Ø³ÙˆØ¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯Ø› Ø²ÛŒØ±Ø§ Ù…ØªÙ† Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡ Ù…Ø¨Ù†Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø±Ø§Ù…Ù¾Øª Ø®Ù„Ø§Ù‚Ø§Ù†Ù‡ Ùˆ Ø¯Ø± Ù†Ù‡Ø§ÛŒØª ØªÙˆÙ„ÛŒØ¯ ØªØµÙˆÛŒØ± Ùˆ ÙˆÛŒØ¯Ø¦Ùˆ Ù‚Ø±Ø§Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯.  

"""

import os
import whisper
import google.generativeai as genai
from pydub import AudioSegment
from google.colab import userdata

# ---------------- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡ ----------------
AUDIO_INPUT_PATH = "song.mp3"
WAV_OUTPUT_PATH = "input.wav"
LYRICS_PATH = "lyrics.txt"
PROMPT_PATH = "video_prompt.txt"
GEMINI_API_KEY = userdata.get("GEMINI_API_KEY")

# ---------------- ØªØ¨Ø¯ÛŒÙ„ mp3 Ø¨Ù‡ wav ----------------
def convert_mp3_to_wav(input_path, output_path):
    print("Converting from MP3 to WAV...")
    try:
        sound = AudioSegment.from_file(input_path)
        sound.export(output_path, format="wav")
        print("Conversion completed.")
        return True
    except Exception as e:
        print(f"Error during MP3 to WAV conversion: {e}")
        return False

# ---------------- Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø¨Ø§ Whisper ----------------
def extract_lyrics(wav_path):
    print("Extracting text with Whisper...")
    try:
        model = whisper.load_model("base")
        result = model.transcribe(wav_path)
        text = result["text"]
        print("Text extracted.")
        return text.strip()
    except Exception as e:
        print(f"Error during text extraction with Whisper: {e}")
        return None

# ---------------- ØªÙˆÙ„ÛŒØ¯ Ù¾Ø±Ø§Ù…Ù¾Øª Ø¨Ø§ Gemini ----------------
def generate_prompt_from_lyrics(lyrics, api_key):
    print("Producing prompts with Gemini...")
    if not api_key:
        print("GEMINI_API_KEY is not set.")
        return None

    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel("models/gemini-2.0-flash")

        system_prompt = f"""
You are a creative assistant. Based on the lyrics of the song below, create a cinematic script for a text-to-video model. The script should clearly describe the mood, visual style, colors, environment, and emotions.

Lyrics:

{lyrics}

Write it creatively and concisely, suitable for AI video production tools like Stable Video or ModelScope, in a few lines and short.

"""
        response = model.generate_content(system_prompt)
        print("Prompt generated.")
        return response.text.strip()
    except Exception as e:
        print(f"Error during prompt generation with Gemini: {e}")
        return None

# ---------------- Ø§Ø¬Ø±Ø§ÛŒ Ø§ØµÙ„ÛŒ ----------------
def main():
    if not os.path.exists(AUDIO_INPUT_PATH):
        print(f"Audio file '{AUDIO_INPUT_PATH}' not found.")
        return

    # Ù…Ø±Ø­Ù„Ù‡ 1: ØªØ¨Ø¯ÛŒÙ„ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ
    if not convert_mp3_to_wav(AUDIO_INPUT_PATH, WAV_OUTPUT_PATH):
        return

    # Ù…Ø±Ø­Ù„Ù‡ 2: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ†
    lyrics = extract_lyrics(WAV_OUTPUT_PATH)
    if not lyrics:
        return

    with open(LYRICS_PATH, "w", encoding="utf-8") as f:
        f.write(lyrics)

    # Ù…Ø±Ø­Ù„Ù‡ 3: ØªÙˆÙ„ÛŒØ¯ Ù¾Ø±Ø§Ù…Ù¾Øª
    prompt = generate_prompt_from_lyrics(lyrics, GEMINI_API_KEY)
    if not prompt:
        print("\nCould not generate prompt. Exiting.")
        return

    # Ù…Ø±Ø­Ù„Ù‡ 4: Ø°Ø®ÛŒØ±Ù‡ Ù¾Ø±Ø§Ù…Ù¾Øª
    with open(PROMPT_PATH, "w", encoding="utf-8") as f:
        f.write(prompt)

    print(f"\n--- Prompt generated by Gemini ---\n{prompt}\n----------------------------------")
    print("\nEverything is ready. Lyrics and prompt saved.")

if __name__ == "__main__":
    main()

"""## ğŸ“Œ Ú¯Ø§Ù… Ø¯ÙˆÙ…: ØªÙˆÙ„ÛŒØ¯ ØªØµÙˆÛŒØ± Ø§ÙˆÙ„ÛŒÙ‡

Ø¯Ø± Ø§ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ØŒ Ù…ØªÙ†ÛŒ Ú©Ù‡ ØªÙˆØ³Ø· Ù…Ø¯Ù„ Ø¬ÙÙ…ÛŒÙ†ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø´Ø¹Ø§Ø± Ù…ÙˆØ³ÛŒÙ‚ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ Ø¨ÙˆØ¯ØŒ
Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù¾Ø±Ø§Ù…Ù¾Øª ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ù‡ Ù…Ø¯Ù„ Ø§Ø³ØªÛŒØ¨Ù„ Ø¯ÛŒÙÛŒÙˆØ´Ù† Ø§ÛŒÚ©Ø³â€ŒØ§Ù„ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯.
Ø§ÛŒÙ† Ù…Ø¯Ù„ ÙˆØ¸ÛŒÙÙ‡ Ø¯Ø§Ø´Øª ØªØ§ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªÙˆØµÛŒÙØ§Øª Ù…ØªÙ†ÛŒØŒ ÛŒÚ© ØªØµÙˆÛŒØ± Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ø§ Ú©ÛŒÙÛŒØª Ø¨Ø§Ù„Ø§ Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù†Ø¯.

ØªØµÙˆÛŒØ± Ø¨Ù‡ Ø¯Ø³Øª Ø¢Ù…Ø¯Ù‡ Ø¯Ø± Ø§ÛŒÙ† ÙØ±Ø¢ÛŒÙ†Ø¯ØŒ Ù†Ù‚Ø·Ù‡ Ø´Ø±ÙˆØ¹ Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ ÙˆÛŒØ¯Ø¦Ùˆ Ù…Ø­Ø³ÙˆØ¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯.
Ø¯Ø± Ø§ÛŒÙ† Ú¯Ø§Ù… Ù‡Ù…Ú†Ù†ÛŒÙ† ØªÙ†Ø¸ÛŒÙ…Ø§ØªÛŒ Ù…Ø§Ù†Ù†Ø¯ Ø§Ù†Ø¯Ø§Ø²Ù‡ ØªØµÙˆÛŒØ± Ùˆ Ú©Ù†ØªØ±Ù„ Ø¨Ø°Ø± ØªØµØ§Ø¯ÙÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯
ØªØ§ ØªÚ©Ø±Ø§Ø±Ù¾Ø°ÛŒØ±ÛŒ Ùˆ ÛŒÚ©Ù¾Ø§Ø±Ú†Ú¯ÛŒ Ø¯Ø± Ù†ØªØ§ÛŒØ¬ ØªØ¶Ù…ÛŒÙ† Ú¯Ø±Ø¯Ø¯.

Ø¯Ø± Ù†ØªÛŒØ¬Ù‡ØŒ Ø®Ø±ÙˆØ¬ÛŒ Ø§ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ ÛŒÚ© ØªØµÙˆÛŒØ± Ø§Ø³Øª Ú©Ù‡ Ø§Ø±ØªØ¨Ø§Ø· Ù…Ø³ØªÙ‚ÛŒÙ…ÛŒ Ø¨Ø§ ÙØ¶Ø§ÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ
Ø§Ø´Ø¹Ø§Ø± Ù…ÙˆØ³ÛŒÙ‚ÛŒ Ø¯Ø§Ø±Ø¯ Ùˆ Ù…Ø¨Ù†Ø§ÛŒ Ú¯Ø§Ù… Ø³ÙˆÙ… ÛŒØ¹Ù†ÛŒ ØªÙˆÙ„ÛŒØ¯ ÙˆÛŒØ¯Ø¦Ùˆ Ù‚Ø±Ø§Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯.
"""

# @title Default title text
!pip install diffusers transformers accelerate torch torchvision imageio[ffmpeg]
!pip install --upgrade Pillow

import os
import torch
from diffusers import DiffusionPipeline
from PIL import Image

# ---------------- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡ ----------------
PROMPT_PATH = "video_prompt.txt"
INITIAL_IMAGE_PATH = "initial_image.png"

# ---------------- ØªÙˆÙ„ÛŒØ¯ ØªØµÙˆÛŒØ± Ø¨Ø§ Stable Diffusion XL ----------------
def generate_initial_image(prompt, output_path):
    print("Generating initial image with Stable Diffusion XL...")
    try:
        # Load the text-to-image model (SDXL)
        txt2img_pipe = DiffusionPipeline.from_pretrained(
            "stabilityai/stable-diffusion-xl-base-1.0",
            torch_dtype=torch.float16,
            variant="fp16"
        )
        txt2img_pipe.to("cuda")

        # Generate the initial image from the prompt
        generator = torch.manual_seed(42)
        initial_image = txt2img_pipe(prompt=prompt, generator=generator).images[0]
        initial_image.save(output_path)
        print("Initial image generated and saved.")

        # Clear GPU memory
        del txt2img_pipe
        torch.cuda.empty_cache()

        return True
    except Exception as e:
        print(f"Error during image generation: {e}")
        return False

# ---------------- Ø§Ø¬Ø±Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ ØªØµÙˆÛŒØ± Ø§ÙˆÙ„ÛŒÙ‡ ----------------
def main_image_generation():
    if not os.path.exists(PROMPT_PATH):
        print(f"Prompt file '{PROMPT_PATH}' not found. Please run the first cell to generate the prompt.")
        return

    with open(PROMPT_PATH, "r", encoding="utf-8") as f:
        prompt = f.read()

    print(f"\n--- Using saved prompt to generate initial image ---\n{prompt}\n----------------------------------")

    if generate_initial_image(prompt, INITIAL_IMAGE_PATH):
        print("\nInitial image generation completed successfully.")
    else:
        print("\nInitial image generation failed.")

if __name__ == "__main__":
    main_image_generation()

"""##ğŸ“Œ Ú¯Ø§Ù… Ø³ÙˆÙ…: ØªÙˆÙ„ÛŒØ¯ ÙˆÛŒØ¯Ø¦Ùˆ

Ø¯Ø± Ø§ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ØŒ ØªØµÙˆÛŒØ± Ø§ÙˆÙ„ÛŒÙ‡ ØªÙˆÙ„ÛŒØ¯Ø´Ø¯Ù‡ ØªÙˆØ³Ø· Ù…Ø¯Ù„ Ø§Ø³ØªÛŒØ¨Ù„ Ø¯ÛŒÙÛŒÙˆØ´Ù† Ø§ÛŒÚ©Ø³â€ŒØ§Ù„ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ù‡ Ù…Ø¯Ù„ Ø§Ø³ØªÛŒØ¨Ù„ ÙˆÛŒØ¯Ø¦Ùˆ Ø¯ÛŒÙÛŒÙˆØ´Ù† Ø¯Ø§Ø¯Ù‡ Ø´Ø¯ ØªØ§ ÛŒÚ© ÙˆÛŒØ¯Ø¦Ùˆ Ú©ÙˆØªØ§Ù‡ Ø§ÛŒØ¬Ø§Ø¯ Ú¯Ø±Ø¯Ø¯.
Ø§ÛŒÙ† Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªÙˆØµÛŒÙØ§Øª Ù¾Ø±Ø§Ù…Ù¾Øª Ùˆ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ØªØµÙˆÛŒØ± Ø§ÙˆÙ„ÛŒÙ‡ØŒ ÙØ±ÛŒÙ…â€ŒÙ‡Ø§ÛŒ Ù…ØªÙˆØ§Ù„ÛŒ Ùˆ Ø­Ø±Ú©Øªâ€ŒÙ‡Ø§ÛŒ Ø·Ø¨ÛŒØ¹ÛŒ Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

ØªÙ†Ø¸ÛŒÙ…Ø§ØªÛŒ Ù…Ø§Ù†Ù†Ø¯ ØªØ¹Ø¯Ø§Ø¯ ÙØ±ÛŒÙ…â€ŒÙ‡Ø§ØŒ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ùˆ Ø±Ø²ÙˆÙ„ÙˆØ´Ù† ØªØµÙˆÛŒØ±ØŒ Ùˆ Ø´Ø¯Øª Ù†ÙˆÛŒØ² Ø­Ø±Ú©ØªÛŒ Ø§Ø¹Ù…Ø§Ù„ Ø´Ø¯ ØªØ§ ÙˆÛŒØ¯Ø¦Ùˆ Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø² Ù†Ø¸Ø± Ø¨ØµØ±ÛŒ Ù‡Ù…Ø§Ù‡Ù†Ú¯ Ùˆ Ø±ÙˆØ§Ù† Ø¨Ø§Ø´Ø¯.
Ø®Ø±ÙˆØ¬ÛŒ Ø§ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ØŒ ÛŒÚ© ÙˆÛŒØ¯Ø¦Ùˆ Ù‡Ù†Ø±ÛŒ Ùˆ Ø®Ù„Ø§Ù‚Ø§Ù†Ù‡ Ø§Ø³Øª Ú©Ù‡ Ø¨ÛŒØ§Ù†Ú¯Ø± ÙØ¶Ø§ÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ùˆ Ø§Ø­Ø³Ø§Ø³ÛŒ Ø§Ø´Ø¹Ø§Ø± Ù…ÙˆØ³ÛŒÙ‚ÛŒ Ø¨ÙˆØ¯Ù‡ Ùˆ Ù†ØªÛŒØ¬Ù‡ Ù†Ù‡Ø§ÛŒÛŒ Ù¾Ø±ÙˆÚ˜Ù‡ Ø±Ø§ ØªØ´Ú©ÛŒÙ„ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.

âŒ**Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø§ÛŒÙ† Ù‚Ø³Ù…Øª Ù…ÛŒØ²Ø§Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ÛŒ Ú¯Ø±Ø§ÙÛŒÚ©ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ ØªÙˆØ³Ø· Ú¯ÙˆÚ¯Ù„ Ú©Ù„Ø¨ Ø¨ÙˆØ¯Ù‡ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ ÙˆÛŒØ¯Ø¦Ùˆ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø®Ø±ÙˆØ¬ÛŒ Ø±ÙˆÛŒ Ú©ÛŒÙÛŒØª Ù¾Ø§ÛŒÛŒÙ† ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯Ù‡ Ø§Ø³ØªâŒ**
"""

import os
import torch
from diffusers import StableVideoDiffusionPipeline
from diffusers.utils import load_image, export_to_video
from PIL import Image
import imageio

# ---------------- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡ ----------------
INITIAL_IMAGE_PATH = "initial_image.png"
VIDEO_OUTPUT_PATH = "generated_video.mp4"

# ---------------- ØªÙˆÙ„ÛŒØ¯ ÙˆÛŒØ¯ÛŒÙˆ Ø¨Ø§ Stable Video Diffusion ----------------
def generate_video_from_image(image_path, output_path):
    print("Generating video from the initial image with Stable Video Diffusion...")
    try:
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø³Ø§Ø®Øª ÙˆÛŒØ¯ÛŒÙˆ
        pipe = StableVideoDiffusionPipeline.from_pretrained(
            "stabilityai/stable-video-diffusion-img2vid-xt",
            torch_dtype=torch.float16,
            variant="fp16"
        )
        pipe.to("cuda")

        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªØµÙˆÛŒØ± Ø§ÙˆÙ„ÛŒÙ‡
        initial_image = load_image(image_path)

        # --- ØªØºÛŒÛŒØ± Ø§ÙˆÙ„: Ú©Ø§Ù‡Ø´ Ø§Ø¨Ø¹Ø§Ø¯ ØªØµÙˆÛŒØ± ---
        # Ù…Ø§ Ø±Ø²ÙˆÙ„ÙˆØ´Ù† Ø±Ùˆ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ Ù…ØµØ±Ù Ø­Ø§ÙØ¸Ù‡ Ú©Ù…ØªØ± Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
        target_size = 512
        width, height = initial_image.size
        if height > width:
            new_height = target_size
            new_width = int(new_height * width / height)
        else:
            new_width = target_size
            new_height = int(new_width * height / width)

        new_width = new_width - new_width % 8
        new_height = new_height - new_height % 8

        initial_image = initial_image.resize((new_width, new_height))
        print(f"Image resized to: {initial_image.size}")


        # ØªÙˆÙ„ÛŒØ¯ ÙØ±ÛŒÙ…â€ŒÙ‡Ø§ÛŒ ÙˆÛŒØ¯ÛŒÙˆ
        # --- ØªØºÛŒÛŒØ± Ø¯ÙˆÙ…: Ú©Ø§Ù‡Ø´ decode_chunk_size ---
        video_frames = pipe(
            initial_image,
            num_frames=25,
            decode_chunk_size=4, # Ø§ÛŒÙ† Ø¹Ø¯Ø¯ Ø§Ø² 8 Ø¨Ù‡ 4 Ú©Ø§Ù‡Ø´ ÛŒØ§ÙØª
            motion_bucket_id=127,
            noise_aug_strength=0.1
        ).frames[0]

        print("Video frames generated. Saving the video...")

        # Ø°Ø®ÛŒØ±Ù‡ ÙØ±ÛŒÙ…â€ŒÙ‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª ÙØ§ÛŒÙ„ ÙˆÛŒØ¯ÛŒÙˆÛŒÛŒ mp4
        export_to_video(video_frames, output_path, fps=7)

        # Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡ GPU
        del pipe
        torch.cuda.empty_cache()

        print(f"Video saved successfully at: {output_path}")
        return True

    except Exception as e:
        print(f"Error during video generation: {e}")
        return False

# ---------------- Ø§Ø¬Ø±Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ ÙˆÛŒØ¯ÛŒÙˆ ----------------
def main_video_generation():
    if not os.path.exists(INITIAL_IMAGE_PATH):
        print(f"Initial image '{INITIAL_IMAGE_PATH}' not found. Please run the previous cells first.")
        return

    if generate_video_from_image(INITIAL_IMAGE_PATH, VIDEO_OUTPUT_PATH):
        print("\nVideo generation completed successfully.")
    else:
        print("\nVideo generation failed.")

if __name__ == "__main__":
    main_video_generation()